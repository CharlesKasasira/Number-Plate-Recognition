# -*- coding: utf-8 -*-
"""Number_plate_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I7BdVnFnpb_cK79W2tDSBQBe1wCrbwQo

# NUMBER PLATE RECOGNITION USING ARTIFICIAL INTELLENGENCE

By **Kasasira Charles Derrick**<br/>
21/U/05662/EVE<br />
Student Makerere University


### Objective:
The objective of this notebook to train the model for the Number Plate Reconition System
"""

#importing the dataset from google drive.
from google.colab import drive

drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

"""## Importing Libraries"""

pip install pytesseract

pip install Image

import cv2
import numpy as np
import os
import pandas as pd
import tensorflow as tf
import pytesseract as pt
from PIL import Image
import plotly.express as px
import matplotlib.pyplot as plt
import xml.etree.ElementTree as xet

from glob import glob
from skimage import io
from shutil import copy
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.layers import Dense, Dropout, Flatten, Input
from tensorflow.keras.preprocessing.image import load_img, img_to_array

"""## Preprocessing the Dataset"""

path = glob('/content/drive/MyDrive/Number_Plate_Dateset/images/*.xml')
labels_dict = dict(filepath=[],xmin=[],xmax=[],ymin=[],ymax=[])
for filename in path:

    info = xet.parse(filename)
    root = info.getroot()
    member_object = root.find('object')
    labels_info = member_object.find('bndbox')
    xmin = int(labels_info.find('xmin').text)
    xmax = int(labels_info.find('xmax').text)
    ymin = int(labels_info.find('ymin').text)
    ymax = int(labels_info.find('ymax').text)

    labels_dict['filepath'].append(filename)
    labels_dict['xmin'].append(xmin)
    labels_dict['xmax'].append(xmax)
    labels_dict['ymin'].append(ymin)
    labels_dict['ymax'].append(ymax)

df = pd.DataFrame(labels_dict)
df.to_csv('labels.csv',index=False)
df.head()

filename = df['filepath'][0]
def getFilename(filename):
    filename_image = xet.parse(filename).getroot().find('filename').text
    filepath_image = os.path.join('/content/drive/MyDrive/Number_Plate_Dateset/images',filename_image)
    return filepath_image
getFilename(filename)

image_path = list(df['filepath'].apply(getFilename))
image_path[:10]

"""Data"""

file_path = image_path[1] #path of our image N2.jpeg
# img = cv2.imread(file_path) #read the image
# xmin-1804/ymin-1734/xmax-2493/ymax-1882 
# img = io.imread(file_path) #Read the image
img = Image.open('/content/drive/MyDrive/Number_Plate_Dateset/images/N1.jpeg') #read the image using Pillow
fig = px.imshow(img)
fig.update_layout(width=600, height=500, margin=dict(l=10, r=10, b=10, t=10),xaxis_title='Figure 8 - N2.jpeg with bounding box')
fig.add_shape(type='rect',x0=1804, x1=2493, y0=1734, y1=1882, xref='x', yref='y',line_color='cyan')

#Targeting all our values in array selecting all columns
labels = df.iloc[:,1:].values
data = []
output = []
for ind in range(len(image_path)):
    image = image_path[ind]
    img_arr = cv2.imread(image)
    h,w,d = img_arr.shape
    # Prepprocesing
    load_image = load_img(image,target_size=(224,224))
    load_image_arr = img_to_array(load_image)
    norm_load_image_arr = load_image_arr/255.0 # Normalization
    # Normalization to labels
    xmin,xmax,ymin,ymax = labels[ind]
    nxmin,nxmax = xmin/w,xmax/w
    nymin,nymax = ymin/h,ymax/h
    label_norm = (nxmin,nxmax,nymin,nymax) # Normalized output
    # Append
    data.append(norm_load_image_arr)
    output.append(label_norm)

# Convert data to array
X = np.array(data,dtype=np.float32)
y = np.array(output,dtype=np.float32)

# Split the data into training and testing set using sklearn.
x_train,x_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=0)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

inception_resnet = InceptionResNetV2(weights="imagenet",include_top=False, input_tensor=Input(shape=(224,224,3)))
# ---------------------
headmodel = inception_resnet.output
headmodel = Flatten()(headmodel)
headmodel = Dense(500,activation="relu")(headmodel)
headmodel = Dense(250,activation="relu")(headmodel)
headmodel = Dense(4,activation='sigmoid')(headmodel)


# ---------- model
model = Model(inputs=inception_resnet.input,outputs=headmodel)

# Complie model
model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))
model.summary()

model.save('./object_detection.h5')

"""Predication"""

# Load model
model = tf.keras.models.load_model('./object_detection.h5')
print('Model loaded Sucessfully')

path = '/content/drive/MyDrive/Number_Plate_Dateset/TEST/TEST.jpeg'
image = load_img(path) # PIL object
image = np.array(image,dtype=np.uint8) # 8 bit array (0,255)
image1 = load_img(path,target_size=(224,224))
image_arr_224 = img_to_array(image1)/255.0  # Convert into array and get the normalized output

# Size of the orginal image
h,w,d = image.shape
print('Height of the image =',h)
print('Width of the image =',w)

dtrain = os.listdir('/content/drive/MyDrive/Number_Plate_Dateset/images')
dtest = os.listdir('/content/drive/MyDrive/Number_Plate_Dateset/images')

TRAIN_PATH = '/content/drive/MyDrive/Number_Plate_Dateset/images'
TEST_PATH = '/content/drive/MyDrive/Number_Plate_Dateset/image/test'

len(dtrain)

img_path = []
txt_path = []
for x in range(len(dtrain)):
    if dtrain[x].endswith('.jpeg'):
        suf = dtrain[x][:-5]
        img_path.append(dtrain[x])
        txt_path.append(suf + '.txt')

img_test_path = []
txt_test_path = []
for x in range(len(dtest)):
    if dtest[x].endswith('.jpeg'):
        suf = dtest[x][:-5]
        img_test_path.append(dtest[x])
        txt_test_path.append(suf + '.txt')

data = []
output = []
for i in range(len(img_path)):
    image = os.path.join(TRAIN_PATH, img_path[i])
    img_arr = cv2.imread(image)
    load_image = load_img(image,target_size=(224,224))
    load_image_arr = img_to_array(load_image)
    norm_load_image_arr = load_image_arr/255.0 # Normalization
    # Normalization to labels
    lab = img_path[i][:-5]
    with open(os.path.join(TRAIN_PATH, lab+'.txt'), 'r') as file:
        doc = file.read()
        file.close()
    cases = doc.split('\n')
    box = []
    for case in cases:
        nxmin,nxmax,nymin,nymax = case.split(' ')[1:]
        label_norm = (float(nxmin),float(nxmax),float(nymin),float(nymax))
        box.append(label_norm)
    data.append(norm_load_image_arr)
    output.append(box)

data_test = []
output_test = []
for i in range(len(img_test_path)):
    image = os.path.join(TEST_PATH, img_test_path[i])
    img_arr = cv2.imread(image)
    load_image = load_img(image,target_size=(224,224))
    load_image_arr = img_to_array(load_image)
    norm_load_image_arr = load_image_arr/255.0 # Normalization
    # Normalization to labels
    lab = img_test_path[i][:-5]
    with open(os.path.join(TEST_PATH, lab+'.txt'), 'r') as file:
        doc = file.read()
        file.close()
    cases = doc.split('\n') 
    box = []
    for case in cases:
        nxmin,nxmax,nymin,nymax = case.split(' ')[1:]
        label_norm = (float(nxmin),float(nxmax),float(nymin),float(nymax))
        box.append(label_norm)
    data_test.append(norm_load_image_arr)
    output_test.append(box)

x_train,x_test,y_train,y_test = np.array(data,dtype=np.float32), np.array(data_test,dtype=np.float32), np.array(output,dtype=np.float32), np.array(output_test,dtype=np.float32)

inception_resnet = InceptionResNetV2(weights="imagenet",include_top=False, input_tensor=Input(shape=(224,224,3)))
# ---------------------
headmodel = inception_resnet.output
headmodel = Flatten()(headmodel)
headmodel = Dense(500,activation="relu")(headmodel)
headmodel = Dense(250,activation="relu")(headmodel)
headmodel = Dense(4,activation='sigmoid')(headmodel)


# ---------- model
model = Model(inputs=inception_resnet.input,outputs=headmodel)

model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['accuracy'])

history = model.fit(x=x_train,y=y_train,batch_size=20,epochs=20,
                    validation_data=(x_test,y_test))

history = model.fit(x=x_train,y=y_train,batch_size=20,epochs=20,
                    validation_data=(x_test,y_test))

# print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.savefig('accuracy.png')
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.savefig('loss.png')